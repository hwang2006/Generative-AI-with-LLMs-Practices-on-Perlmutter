# Generative-AI-with-LLMs-Practices-on-Perlmutter
This repository is intended to share Generative AI with Large Language Models (LLMs) practices that I have performed on Perlmutter. The Jupyter codes are originated from the [online "Generative AI with LLMs" course](https://www.coursera.org/learn/generative-ai-with-llms) offered by the DeepLeanring.AI. Generative AI with LLMs refers to the use of large language models like GPT-3 for generating human-like content, spaning text, images and even code. LLMs are trained on a vast amount of data and code, and usually carefully prompt-engineered or fine-tuned to suit specific downstream tasks such as Chatbots, Translation, Question Answering and Summarization.

**Contents**
* [NERSC Perlmutter Supercomputer](#nersc-perlmutter-supercomputer)


## NERSC Perlmutter Supercomputer
[Perlmutter](https://docs.nersc.gov/systems/perlmutter/), located at [NERSC](https://www.nersc.gov/) in [Lawrence Berkeley National Laboratory](https://www.lbl.gov/), is a HPE Cray EX supercomputer with ~1,500 AMD Milan CPU nodes and ~6000 Nvidia A100 GPUs (4 GPUs per node). It debuted as the world 5th fastest supercomputer in the Top500 list in June 2021. Please refer to [Perlmutter Architecture](https://docs.nersc.gov/systems/perlmutter/architecture/) for the architecutural details of Perlmutter including system specifications, system performance, node specifications and interconnect. [Slurm](https://slurm.schedmd.com/) is adopted for cluster/resource management and job scheduling. 

<p align="center"><img src="https://user-images.githubusercontent.com/84169368/218645916-30e920b5-b2cf-43ad-9f13-f6a2568c0e37.jpg" width=550/></p>



